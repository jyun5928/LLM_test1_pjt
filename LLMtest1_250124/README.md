# LLM_test1_pjt
# Transformer 모델
트랜스포머(Transformer)는 구글이 자연어처리를 위해 2017년 발표한 모델로,
문장 속 단어와 같은 순차 데이터 내의 관계를 추적해 맥락과 의미를 학습하는 신경망이다.
또한 병렬 처리와 자기 주의 메커니즘을 통해 입력 데이터를 한 번에 처리할 수 있다.


## 1. Transformer의 구성요소
### 1. 인코더(Encoder)와 디코더(Decoder)
* **인코더 (Encoder)**: 입력 문장을 처리하여 고차원 벡터로 변환
    * **인코더의 구조** 
    1. **입력 임베딩 (Input Embedding)**: 단어를 벡터로 변환
    2. **위치 인코딩 (Positional Encoding)**: 단어 순서를 반영
    3. **멀티-헤드 자기 주의 (Multi-Head Self-Attention)**: 단어 간 관계 파악
    4. **피드포워드 신경망 (Feed-Forward Neural Network)**: 단어 표현 변환
    5. **출력 (Output)**: 변환된 벡터 출력 

* **디코더 (Decoder)**: 인코더에서 전달받은 정보로부터 타겟 문장을 생성
    * 인코더-디코더 주의 (Encoder-Decoder Attention) 메커니즘이 존재

### 2. 어텐션
**1. 자기 어텐션(Self-Attention)**
: 자기 어텐션은 한 문장 내의 각 단어가 다른 단어들과의 관계를 파악하는 방식. 
인코더가 입력 문장을 이해하기 위한 인코더 내 셀프 어텐션과 디코더가 자신이 생성하고 있는 문장을 잘 이해하기 위한 디코너 내 셀프 어텐션이 존재

**2. 인코더-디코더 어텐션 (Encoder-Decoder Attention)**
: 디코더가 인코더에서 나온 정보를 참조하면서, 필요한 부분에 집중할 수 있게 돕는 어텐션

**3. 멀티-헤드 어텐션 (Multi-Head Attention)**
: 여러 개의 어텐션을 동시에 계산하여, 모델이 다양한 관점에서 정보를 처리할 수 있도록 하는 기법. 여러 개의 어텐션 헤드를 통해 각기 다른 관계를 학습하고, 이를 결합하여 더 풍부한 표현을 생성.

### 3. 위치 인코딩 (Positional Encoding)
각 단어에 고유한 위치 정보를 추가하여 단어 순서를 모델에 전달.
주로 **사인(Sine)**과 코사인(Cosine) 함수를 이용해 생성.

### 4. 피드포워드 신경망 (Feed-Forward Neural Network)
각 단어의 표현을 변환하는 역할. 주로 두 개의 선형 변환과 활성화 함수로 구성.

트랜스포머는 어텐션을 통해 문장 길이에 대한 제약을 극복, 입력 문장과 생성 문장에 대한 이해를 넓혔다. 그리고 모든 연산 과정에서 입력값과 가중치간 어마어마한 행렬 연산이 이뤄지는 데 이것을 모두 병렬적으로 처리함으로써 효율적인 연산이 가능하게 되었다. 이로 인해 대규모 데이터셋을 사전에 학습해 뛰어난 성능을 보이는 GPT (Generative Pre-trained Transformer)와 같은 대규모 언어모델의 기반이 되었다고 할 수 있다.

## 2. Transformer 모델의 장점
### 1. 병렬 처리
attention을 통해 문장 길이에 대한 제약을 극복, 모든 연산 과정에서 입력값과 가중치간 어마어마한 행렬 연산이 이뤄지는 데 이것을 모두 병렬적으로 동시에 처리함으로써 대규모 데이터의 효율적인 연산이 가능하게 됐다.

### 2. 장기 의존성 문제 해결
attention 메커니즘을 사용해 모든 단어 간의 관계를 동시에 파악할 수 있기 때문에, 장기 의존성 문제를 자연스럽게 해결할 수 있다.
즉, 문장의 첫 번째 단어와 마지막 단어 간의 관계도 쉽게 파악할 수 있다.

### 3. 확장성
멀티-헤드 어텐션(multi-head attention)과 병렬 처리를 통해 모델 확장성이 뛰어나다. 모델을 쉽게 확장하거나 축소할 수 있어, 더 복잡한 모델을 학습하거나 다양한 작업에 적용하기에 적합.

### 4. 유연한 아키텍처 (Flexible Architecture)
Transformer는 인코더와 디코더로 나뉘며, 각각 독립적으로도 사용될 수 있다. 
또한, Transformer는 다양한 하이퍼파라미터와 구조적 변경에 따라 다양한 모델을 만들 수 있어, 특정 작업에 맞춰 최적화가 가능.